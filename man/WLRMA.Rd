% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wlrma.R
\name{WLRMA}
\alias{WLRMA}
\title{Weighted Low-Rank Matrix Approximation}
\usage{
WLRMA(
  M,
  W = NULL,
  type = "soft",
  parameter,
  method = "svd",
  initialization = NULL,
  acc_method = "baseline",
  acc_parameters = list(depth = 3, delay = 0, guarded = FALSE, reg_depth = 3, gamma =
    0),
  threshold = 1e-08,
  max_iter = 100,
  verbose = FALSE
)
}
\arguments{
\item{M}{a rectangular \eqn{n x m} matrix the approximation should be computed for.}

\item{W}{a rectangular \eqn{n x m} matrix containing non-negative weights. By default \code{W = NULL}, i.e. non-weighted problem is solved.}

\item{type}{the problem type. Default value is \code{type = "soft"}, i.e. the WLRMA convex relaxation (2) is solved. Use \code{type = "hard"} to solve the non-convex WLRMA problem (1).}

\item{parameter}{the problem hyperparameter. For \code{type = "hard"}, i.e. non-convex WLRMA, \code{parameter} corresponds to the solution rank \eqn{k}. For \code{type = "soft"}, i.e. the convex relaxation, \code{parameter} corresponds to \eqn{\lambda}.}

\item{method}{the method the solution is acheved. By default \code{method = "svd"}, thus the algorithm based on projected gradient descent
and utilizing SVD at each iteration is used. If \code{method = "als"} the alternating least squares algorithm is used to solve the ploblem.}

\item{initialization}{a list containing initialization for the algorithm. If PGD-type algorithm is used the list should contain the inialization for \eqn{X}, i.e. 
\code{initialization = list(X = ...)}.
If ALS-type algorithm is used the list should contain the initialization for \eqn{A} and \eqn{B}, i.e. 
\code{initialization = list(A = ..., B = ...)}. 
The default value is \code{NULL} and random initialization is applied.}

\item{acc_method}{the acceleration method applied to improve the convergence speed. By default \code{acc_method = "baseline"} which corresponds to no acceleration. 
One can set \code{acc_method = "nesterov"} to use Nesterov acceleration. Alternatively, set \code{acc_method = "anderson"} to apply Anderson acceleration.
Option \code{acc_method = "randerson"} performs regularized Anderson acceleration.}

\item{acc_parameters}{a list containing acceleration parameters. For anderson acceleration set \code{acc_parameter = list(depth = ..., delay = ..., guarded = ...)}.
Here \code{depth} is the anderson depth, \code{delay} is the number of iterations the acceleration should be delayed by and \code{guarded} is logical variable indicating 
if guarded acceleration should be used.  
The default values are \code{depth = 3}, \code{delay = 0} and \code{guarded = FALSE}.
For regularized Anderson acceleration use two extra parameters \code{reg_depth = ...} and \code{gamma = ...} to control the regularization depth, i.e.
and regularization strength.}

\item{threshold}{the convergence threshold. The algorithm stops when the relative change in the loss becomes less than \code{threshold}.
By default \code{threshold = 1e-8}.}

\item{max_iter}{the maximum number of iterations performed. By default \code{max_iter = 100}.}

\item{verbose}{logical variable indicating if progress after each iteration should be printed. By default \code{verbose = FALSE}.}
}
\value{
A list containing:
\itemize{
  \item \code{solution} -- the list containing the solution of the optimization problem. For the PGD-type algorithm \code{solution = list(X = ...)}, 
  for the ALS-type \code{solution = list(A = ..., B = ...)}.
  \item \code{info} -- the data frame containing the algorithm progress while converging. The data frame includes: iteration number, 
  time, value of the loss, rank, relative change in the loss and relative change in the solution.
}
If Anderson acceleration was applied the function also returns
\itemize{
  \item \code{coefs} -- a matrix containing (column-wise) the paths for Anderson coefficients \eqn{\alpha}. 
}
}
\description{
WLRMA function solves a general weighted low-rank matrix approximation problem. Given some matrix \eqn{M} and a matrix of weights \eqn{W} 
(both of size \eqn{n x p}) as well as some rank \eqn{k}, WLRMA seeks for matrix \eqn{X} such that \eqn{rk(X)\le k} and 
that minimizes the weighted frobenious norm
\deqn{||\sqrt W * (M - X)||_F^2           (1)} 
Here \eqn{*} corresponds to the element-wise matrix product.
The corresponding convex relaxation of this problem is 
\deqn{1/2 ||\sqrt W * (M - X)||_F^2 + \lambda||X||_*            (2)}
Here \eqn{||...||_*} refers to the matrix nuclear norm.

These two problems can be solved via an algorithm based on projected gradeint descent (PGD), 
which requires taking singular value decomposition of \eqn{n x p} matrix at each iteration. 
Although when \eqn{n} and \eqn{p} are small this can be quite an efficient approach, in high dimensions calculating SVD could be impossible.
As an alternative for high-dimensional data one can consider the algorithm based on the alternating least squares (ALS). 
This algorithm utilizes the fact that any solution \eqn{X} of rank \eqn{k} can be decomposed in the 
product \eqn{X = AB^T} where \eqn{A} is \eqn{n x k} matrix
and \eqn{B} is \eqn{p x k} matrix. Thus the ALS approach finds \eqn{A} and \eqn{B} that minimize
\deqn{||\sqrt W * (M - AB^T)||_F^2            (3)}
To build the convex relaxation for this problem one can use the fact that for each \eqn{\lambda} there exists 
\eqn{r} such that for all \eqn{k\ge r}
problem (2) is equivalent to minimizing 
\deqn{1/2 ||\sqrt W  * (M - AB^T)||_F^2 + \lambda/2 ||A||_F^2 + \lambda/2 ||B||_F^2         (4)}
with respect to \eqn{A} and \eqn{B}.
}
\examples{
#generate some data
set.seed(1)
n = 1000
p = 100
k = 70
A_true = matrix(rnorm(n * k), n, k)
B_true = matrix(rnorm(p * k), p, k)
noise = matrix(rnorm(n * p, 0, 1), n, p)
M = A_true \%*\% t(B_true) + noise
W = matrix(runif(n * p), n, p)

#solve non-convex WLRMA
type = "hard"
#find solution via PGD
method = "svd"
#set solution rank to 50
k = 50
sol = WLRMA(M, W, type, parameter = k, method, verbose = TRUE)
#plot the loss relative change vs. time
library(ggplot2)
ggplot(sol$info, aes(time, log(delta, 10))) +
geom_line() + ylab(bquote(log(Delta)))+
xlab("iteration")

#solve convex relaxation via PGD
type = "soft"
#set penalty factor to 30
lambda = 30
sol = WLRMA(M, W, type, parameter = lambda, method, verbose = TRUE)
#plot the rank vs. iteration
ggplot(sol$info, aes(iter, rank)) +
geom_line() + xlab("iteration")

#accelerate via Nesterov
nsol = WLRMA(M, W, type, parameter = lambda, method, acc_method = "nesterov", verbose = TRUE)
#compare convergence speed 
df = rbind(data.frame(sol$info, "acc_method" = "baseline"),
          data.frame(nsol$info, "acc_method" = "nesterov"))
ggplot(df, aes(time, log(delta, 10), color = acc_method)) +
 geom_line() + ylab(bquote(log(Delta)))+
 xlab("iteration")
 
#accelerate Anderson and regularized Anderson
 asol = WLRMA(M, W, type, parameter = lambda, method, acc_method = "anderson", verbose = TRUE)
rasol = WLRMA(M, W, type, parameter = lambda, method, acc_method = "randerson", verbose = TRUE)
#compare all four methods
df = rbind(df,
          data.frame(asol$info, "acc_method" = "anderson"),
          data.frame(rasol$info, "acc_method" = "randerson"))
ggplot(df, aes(time, log(delta, 10), color = acc_method)) +
 geom_line() + ylab(bquote(log(Delta)))+
 xlab("iteration")

#now generate large data
set.seed(1)
n = 1000
p = 1000
k = 30
A_true = matrix(rnorm(n * k), n, k)
B_true = matrix(rnorm(p * k), p, k)
noise = matrix(rnorm(n * p, 0, 1), n, p)
M = A_true \%*\% t(B_true) + noise
W = matrix(runif(n * p), n, p)

#solve the WLRMA convex relaxation via PGD method
type = "soft"
method = "svd"
lambda = 100
#set initialization
init = list(X = matrix(rnorm(n * p), n, p))
sol = WLRMA(M, W, type, parameter = lambda, method, initialization = init, verbose = TRUE)

#use ALS to solve the problem
method = "als"
#set initialization
r = 50
init = list(A = matrix(rnorm(n * r), n, r), B = matrix(rnorm(p * r), p, r))
sol.als = WLRMA(M, W, type, parameter = lambda, method, initialization = init, verbose = TRUE)
#compare the convergence speed
df = rbind(data.frame(sol$info, "method" = "svd"),
data.frame(sol.als$info, "method" = "als"))
ggplot(df, aes(time, log(delta, 10), color = method)) +
 geom_line() + ylab(bquote(log(Delta)))+
 xlab("iteration")


}
